# ğŸ“ Student Performance Predictor â€” End-to-End MLOps Pipeline

[![CI](https://github.com/Ishtiaque-h/student-performance-indicator/actions/workflows/ci.yml/badge.svg)](https://github.com/Ishtiaque-h/student-performance-indicator/actions/workflows/ci.yml)
[![Staging Deploy](https://github.com/Ishtiaque-h/student-performance-indicator/actions/workflows/deploy.yml/badge.svg)](https://github.com/Ishtiaque-h/student-performance-indicator/actions/workflows/deploy.yml)
[![CD Production](https://github.com/Ishtiaque-h/student-performance-indicator/actions/workflows/cd-cloudrun.yml/badge.svg?event=push)](https://github.com/Ishtiaque-h/student-performance-indicator/actions/workflows/cd-cloudrun.yml)
[![Retrain](https://github.com/Ishtiaque-h/student-performance-indicator/actions/workflows/retrain.yml/badge.svg)](https://github.com/Ishtiaque-h/student-performance-indicator/actions/workflows/retrain.yml)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)

A **production-grade machine learning system** that predicts student math scores based on demographic and educational factors. This project demonstrates end-to-end MLOps practices including automated retraining, quality gates, staged deployments, and cloud-native architecture.

ğŸ”— **Live Demo**: [https://student-performance-api-654581958038.us-central1.run.app](https://student-performance-api-654581958038.us-central1.run.app)

---

## ğŸ¯ Project Highlights

Focus isn't just a model â€” it's a **complete ML production system**:

- âœ… **10+ ML models** with hyperparameter tuning (RandomizedSearch â†’ GridSearch)
- âœ… **Automated CI/CD** with GitHub Actions (4 workflows)
- âœ… **Staging + Production environments** with manual promotion gates
- âœ… **Weekly automated retraining** with quality thresholds
- âœ… **"Promote, don't retrain"** â€” production always gets the exact staged model
- âœ… **FastAPI REST service** with dynamic schema validation
- âœ… **Cloud-native deployment** on Google Cloud Run
- âœ… **MLflow integration** for experiment tracking
- âœ… **Docker containerization** with multi-stage builds
- âœ… **Workload Identity Federation** â€” no long-lived GCP credentials
- âœ… **Dataset-agnostic design** â€” change one file to use any dataset

---

## ğŸŒŸ Key Differentiators

### **1. "Promote, Don't Retrain" Model Deployment**
Production deployments never retrain from scratch. The model artifact is:
1. Trained once in a controlled environment
2. Validated against a quality gate (RÂ²)
3. Deployed to staging for human review
4. Promoted to production **as the exact same binary** â€” no variance, no surprises

### **2. Dataset-Agnostic Design**
- Change **one file** (`config.py`) to use a different dataset
- API validation, preprocessing, and training adapt automatically
- Validation logic reads from **trained preprocessor** (single source of truth)

### **3. Production-Grade Testing**
- Smoke tests focus on "does the whole pipeline run?"
- Dynamic schema validation prevents invalid inputs
- Post-deploy API smoke tests validate every production release

### **4. MLOps Best Practices**
- Staged deployments (staging â†’ manual review â†’ production)
- Quality gates (RÂ² thresholds, artifact existence checks)
- GCS model registry with versioned run index + promoted pointer
- Automated retraining with human oversight before production

---

## ğŸ“Š Problem Statement

**Question**: How do demographic and educational factors affect student academic performance?

**Dataset**: [Kaggle Student Performance Dataset](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams)
- 1,000 students
- 8 features: gender, race/ethnicity, parental education, lunch type, test prep course, scores
- **Target**: Math score (regression task)
- **Features used**: 5 categorical features (excluded reading/writing scores to prevent data leakage)

---

## ğŸ—ï¸ Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Developer Workflow                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               GitHub Actions (4 Workflows)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ci.yml        â”‚ Lint, Format, Smoke Tests (every PR/push)  â”‚
â”‚  deploy.yml    â”‚ Staging deploy (after CI passes on main)   â”‚
â”‚  retrain.yml   â”‚ Weekly retraining + staging rollout        â”‚
â”‚  cd-cloudrun   â”‚ Production deploy (on v* tag)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Google Cloud Storage (GCS)       â”‚   Artifact Registry  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Model artifacts (model.pkl)        â”‚ â€¢ Docker images      â”‚
â”‚ â€¢ Preprocessor (preprocessor.pkl)    â”‚ â€¢ Tagged by SHA/ver  â”‚
â”‚ â€¢ Run index (latest/<run_id>/)       â”‚                      â”‚
â”‚ â€¢ Promoted pointer (promoted/        â”‚                      â”‚
â”‚     latest_uri.txt)                  â”‚                      â”‚
â”‚ â€¢ MLflow experiment runs             â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Google Cloud Run (Serverless)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Staging:    student-performance-api-staging                â”‚
â”‚  Production: student-performance-api                        â”‚
â”‚  â€¢ Auto-scaling (0â†’N instances)                             â”‚
â”‚  â€¢ HTTPS endpoints                                          â”‚
â”‚  â€¢ Model loaded from GCS at startup (FORCE_MODEL_DOWNLOAD)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ MLOps Pipeline

### **Workflow 1 â€” CI (`ci.yml`)**

Triggered on every pull request and every push to `main`.

```
PR / push to main
    â†“
Lint (ruff) + Format check (black)
    â†“
Smoke tests (pytest -m smoke)
    â†“
âœ… Pass â†’ unblocks staging deploy
âŒ Fail â†’ blocks merge + staging deploy
```

### **Workflow 2 â€” Staging Deploy (`deploy.yml`)**

Triggered only after CI passes on `main` (via `workflow_run`).

```
CI passes on main
    â†“
Train model (all 10 models + hyperparameter tuning)
    â†“
Build & push Docker image (tagged with commit SHA)
    â†“
Deploy to Cloud Run STAGING
```

> This deploy is for **code changes**. The model is retrained fresh to validate the new code works end-to-end with a real model.

### **Workflow 3 â€” Automated Retraining (`retrain.yml`)**

Triggered every Monday at 07:00 UTC (or manually).

```
Schedule: Every Monday 07:00 UTC
    â†“
Train all models with latest data
    â†“
Sync MLflow run to GCS
    â†“
Quality Gate: test_r2 >= 0.10
    â”œâ”€ FAIL â†’ workflow stops (model discarded)
    â””â”€ PASS â†“
Write promoted URI â†’ GCS: promoted/latest_uri.txt
    â†“
Update Cloud Run STAGING with new model (hot-reload)
    â†“
[Human reviews staging predictions & MLflow metrics]
```

### **Workflow 4 â€” Production Deploy (`cd-cloudrun.yml`)**

Triggered by pushing a `v*` tag (e.g. `v3.4.0`).

```
git tag v3.4.0 && git push origin v3.4.0
    â†“
Smoke tests (code quality gate)
    â†“
Read promoted/latest_uri.txt from GCS  â† exact model from staging
    â†“
Build & push Docker image (tagged with version)
    â†“
Verify model.pkl + preprocessor.pkl exist in GCS
    â†“
Deploy to Cloud Run PRODUCTION with promoted model URI
    â†“
Post-deploy smoke tests:
    â”œâ”€ GET /health â†’ must return 200
    â”œâ”€ POST /predict (valid input) â†’ must return 200 + prediction key
    â””â”€ POST /predict (missing field) â†’ must return 422
```

> **Key principle**: No retraining happens here. Production gets the **exact same `.pkl` binary** that was validated on staging. The tag is a human approval gate, not a training trigger.

---

## ğŸ”‘ The "Promote, Don't Retrain" Pattern

This is the most important MLOps design decision in this project:

```
âŒ Naive approach (what most tutorials show):
   push tag â†’ retrain â†’ deploy to prod
   Problem: production model was never validated; training is non-deterministic

âœ… This project:
   retrain â†’ gate â†’ staging â†’ human review â†’ promote same artifact â†’ prod
   Result: production gets the exact model you reviewed on staging
```

The hand-off is a simple GCS pointer:
```
gs://bucket/student-performance/promoted/latest_uri.txt
â†’ contents: gs://bucket/student-performance/latest/20260222T070000Z-abc1234/
```

`retrain.yml` writes it. `cd-cloudrun.yml` reads it. One line. Fully auditable.

---

## ğŸ“Š Exploratory Data Analysis

The `notebooks/` directory contains Jupyter notebooks documenting the data exploration process:

- **EDA_student_performance.ipynb**:
  - Dataset overview and statistics
  - Feature distributions and correlations
  - Missing value analysis
  - Insights that informed feature engineering decisions

This analysis informed key decisions:
- âœ… Dropping reading/writing scores to prevent data leakage
- âœ… Using only categorical features (no numerical scaling needed)
- âœ… Setting RÂ² threshold at 0.10 (conservative â€” reflects limited predictive signal from 5 categorical features alone)

[View EDA Notebook â†’](./notebooks/EDA_student_performance.ipynb)

---

## ğŸ§  Machine Learning

### **Models Evaluated**

| Model | Description | Hyperparams Tuned |
|-------|-------------|-------------------|
| **Linear Regression** | Baseline | â€” |
| **Ridge** | L2 regularization | `alpha` |
| **Lasso** | L1 regularization | `alpha` |
| **KNN** | K-nearest neighbors | `n_neighbors`, `weights`, `p` |
| **Decision Tree** | Single tree | `max_depth`, `criterion` |
| **Random Forest** | Ensemble of trees | `n_estimators`, `max_depth`, `max_features` |
| **AdaBoost** | Boosting ensemble | `n_estimators`, `learning_rate` |
| **Gradient Boosting** | GB ensemble | `n_estimators`, `learning_rate`, `max_depth` |
| **XGBoost** | Advanced GB | `n_estimators`, `learning_rate`, `max_depth`, `subsample` |
| **CatBoost** | Cat-optimized GB | `iterations`, `learning_rate`, `depth` |

### **Hyperparameter Tuning Strategy**

**Two-stage approach**:

1. **Broad Search** (RandomizedSearchCV, 25 iterations):
   - Wide parameter ranges
   - Identifies promising regions quickly

2. **Refined Search** (GridSearchCV):
   - Narrows around best params from stage 1
   - Custom refinement strategies:
     - `float_log`: Multiplicative factors (e.g., 0.5Ã—, 1Ã—, 2Ã—)
     - `int_window`: Additive deltas (e.g., âˆ’50, 0, +50)
     - `categorical`: Discrete choices

### **Model Selection**

- **Scoring metric**: RÂ² (5-fold CV)
- **Selection criterion**: Prefers CV score over test RÂ² (prevents overfitting to test set)
- **Quality gate**: Test RÂ² â‰¥ 0.10 required for staging promotion

### **Preprocessing**

```python
Numerical Features:
  â”œâ”€ Imputation: SimpleImputer(strategy='median')
  â””â”€ Standardization: StandardScaler()

Categorical Features:
  â”œâ”€ Imputation: SimpleImputer(strategy='most_frequent')
  â””â”€ Encoding: OneHotEncoder(handle_unknown='ignore')
```

Output: Sparse matrix (memory-efficient) with automatic densification for models that require dense input (KNN, Decision Trees, Boosting).

---

## ğŸš€ Quick Start

### **Prerequisites**

- Python 3.11+
- Docker (optional, for local testing)
- GCP account (for deployment)

### **Installation**

```bash
# Clone the repository
git clone https://github.com/Ishtiaque-h/student-performance-indicator.git
cd student-performance-indicator

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -e ".[dev,api,ml,mlops]"
```

### **Train Model Locally**

```bash
# Train all models with hyperparameter tuning
python scripts/train_and_publish.py \
  --registry-uri gs://YOUR-BUCKET/student-performance \
  --index-latest

# Artifacts saved to: artifacts/
# - model.pkl
# - preprocessor.pkl
# - model_report.json
# - ingestion_meta.json
```

### **Run API Locally**

```bash
# Start FastAPI server
uvicorn student_performance.api:app --reload --port 8000

# Test health endpoint
curl http://localhost:8000/health

# Make prediction
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "gender": "female",
    "race_ethnicity": "group B",
    "parental_level_of_education": "bachelor'\''s degree",
    "lunch": "standard",
    "test_preparation_course": "none"
  }'
```

### **Run Tests**

```bash
# Smoke tests (fast, ~3 seconds)
pytest -m smoke -v

# All tests
pytest -v

# With coverage
pytest --cov=student_performance --cov-report=html
```

---

## ğŸ“ Project Structure

```
student-performance-indicator/
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ ci.yml                  # Lint, format, smoke tests (every PR + main push)
â”‚   â”œâ”€â”€ deploy.yml              # Staging deploy (after CI passes)
â”‚   â”œâ”€â”€ cd-cloudrun.yml         # Production deploy (on v* tag, promote-don't-retrain)
â”‚   â””â”€â”€ retrain.yml             # Weekly retraining + staging rollout
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/stud.csv            # Source dataset
â”œâ”€â”€ src/student_performance/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ config.py           # Centralized configuration (change dataset here)
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py   # Data loading + splitting
â”‚   â”‚   â”œâ”€â”€ data_transformation.py  # Preprocessing
â”‚   â”‚   â””â”€â”€ model_trainer.py    # Model training + tuning
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ train_pipeline.py   # Training orchestration
â”‚   â”‚   â””â”€â”€ predict_pipeline.py # Inference pipeline
â”‚   â”œâ”€â”€ mlops/
â”‚   â”‚   â””â”€â”€ mlflow_logger.py    # MLflow integration
â”‚   â”œâ”€â”€ registry/
â”‚   â”‚   â””â”€â”€ gcs_registry.py     # GCS artifact management
â”‚   â”œâ”€â”€ api.py                  # FastAPI application
â”‚   â”œâ”€â”€ modeling.py             # Model evaluation
â”‚   â”œâ”€â”€ artifacts_gcs.py        # GCS artifact download
â”‚   â”œâ”€â”€ utils.py                # Utility functions
â”‚   â”œâ”€â”€ logger.py               # Logging setup
â”‚   â””â”€â”€ exception.py            # Custom error handling
â”œâ”€â”€ static/
â”‚   â””â”€â”€ app.js                  # Frontend JavaScript
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html              # Web UI template
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_and_publish.py    # CLI: train + publish to GCS registry
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_smoke.py           # Dataset-agnostic end-to-end tests
â”‚   â””â”€â”€ test_api_validation.py  # Dynamic schema validation tests
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ EDA_student_performance.ipynb
â”‚   â””â”€â”€ model_training.ipynb
â”œâ”€â”€ Dockerfile                  # Multi-stage build
â”œâ”€â”€ pyproject.toml              # Package + extras configuration
â””â”€â”€ README.md
```

---

## ğŸ”§ Configuration

All configuration is centralized in `src/student_performance/components/config.py`:

```python
CONFIG = PipelineConfig(
    dataset=DatasetConfig(
        data_rel_path="data/raw/stud.csv",
        target_col="math_score",
        drop_cols=["reading_score", "writing_score"]
    ),
    split=SplitConfig(test_size=0.2, random_state=42, shuffle=True),
    tuning=TuningConfig(
        cv=5,
        scoring="r2",
        random_n_iter=25,
        random_seed=42,
        prefer_cv_for_selection=True
    ),
    dense_safety=DenseSafetyConfig(
        dense_feature_threshold=5000,
        dense_cell_threshold=5_000_000
    )
)
```

**To use a different dataset**: Update `data_rel_path`, `target_col`, and `drop_cols`. Everything else adapts automatically.

---

## ğŸ“Š API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check + artifact status |
| `GET` | `/` | Web UI (form-based prediction) |
| `GET` | `/schema` | Feature schema (from trained preprocessor) âœ¨ |
| `GET` | `/meta` | Artifact metadata (paths, versions) |
| `POST` | `/predict` | Single prediction (with dynamic validation) âœ¨ |
| `POST` | `/predict_batch` | Batch predictions (with dynamic validation) âœ¨ |

âœ¨ = **Dynamic validation** â€” inputs validated against categories learned during training

### **Example Request**

```bash
curl -X POST https://student-performance-api-654581958038.us-central1.run.app/predict \
  -H "Content-Type: application/json" \
  -d '{
    "gender": "male",
    "race_ethnicity": "group C",
    "parental_level_of_education": "some college",
    "lunch": "free/reduced",
    "test_preparation_course": "completed"
  }'
```

**Response**:
```json
{ "prediction": 68.4 }
```

---

## ğŸ”„ Dynamic Schema Validation

This API validates incoming requests against the **categories learned during training**, not hard-coded values. This makes the system fully dataset-agnostic.

### How It Works

1. **Training**: `OneHotEncoder` learns valid categories from data
2. **API startup**: Loads preprocessor, extracts `encoder.categories_`
3. **Request time**: Validates user input against learned categories

### Example Error Responses

```json
{ "detail": "Empty values not allowed for fields: ['gender']" }
{ "detail": "Invalid value 'non-binary' for field 'gender'. Expected one of: female, male" }
{ "detail": "Missing required fields: ['test_preparation_course']" }
```

---

## ğŸ”’ Production Deployment

### **Infrastructure**

| Component | Technology |
|-----------|------------|
| Serving platform | Google Cloud Run (serverless) |
| Authentication | Workload Identity Federation (no SA keys) |
| Artifact storage | Google Cloud Storage |
| Container registry | Google Artifact Registry |
| Secrets | GitHub Secrets + Environments |
| Experiment tracking | MLflow â†’ GCS |

### **Deployment Process**

```bash
# 1. Retraining runs automatically every Monday
#    â†’ staging is updated automatically

# 2. Review staging
open https://student-performance-api-staging-....run.app

# 3. Promote to production (triggers cd-cloudrun.yml)
git tag v3.4.0
git push origin v3.4.0
```

### **Quality Gates**

| Gate | Where | What |
|------|-------|------|
| Lint + format | CI | Code quality before any deploy |
| Smoke tests (code) | CI + CD | Pipeline runs end-to-end |
| RÂ² â‰¥ 0.10 | `retrain.yml` | Model quality before staging |
| Artifact existence | `cd-cloudrun.yml` | `model.pkl` + `preprocessor.pkl` in GCS |
| Post-deploy smoke | `cd-cloudrun.yml` | Live `/health` + `/predict` + 422 check |

---

## ğŸ“ˆ Monitoring & Observability

### **Current**

- âœ… Post-deploy smoke tests (health + prediction validation)
- âœ… MLflow experiment tracking (synced to GCS per run)
- âœ… Artifact versioning (GCS run index + promoted pointer)
- âœ… Docker image versioning (SHA for staging, semver for production)
- âœ… Deployment logs (Cloud Run)

### **Future Enhancements**

- [ ] Data drift detection
- [ ] Model performance monitoring (production predictions vs. ground truth)
- [ ] Cloud Monitoring alerts
- [ ] A/B testing / canary deployments
- [ ] Slack/email notifications on retrain gate failure

---

## ğŸ§ª Testing Strategy

| Test Type | Coverage | Trigger | Purpose |
|-----------|----------|---------|---------|
| **Smoke tests** | End-to-end pipeline | Every PR + push | Does the pipeline run? |
| **API validation** | Dynamic schema | Every commit | Are inputs validated? |
| **Linting (ruff)** | Code quality | Every commit | Style + correctness |
| **Formatting (black)** | Code style | Every commit | Consistent formatting |
| **Post-deploy smoke** | Live API | After production deploy | Is the deployment healthy? |

### **Running Tests**

```bash
pytest -m smoke -v                              # fast smoke tests
pytest -v                                       # all tests
pytest --cov=student_performance --cov-report=html  # with coverage
pytest tests/test_api_validation.py -v          # schema validation only
```

---

## ğŸ¤ Contributing

This is a portfolio project, but feedback is welcome!

1. Fork the repo
2. Create a feature branch: `git checkout -b feature/my-feature`
3. Commit: `git commit -m 'Add my feature'`
4. Push: `git push origin feature/my-feature`
5. Open a Pull Request

---

## ğŸ“ License

MIT License â€” see [LICENSE](LICENSE) file.

---

## ğŸ‘¤ Author

**Md Ishtiaque Hossain**
- GitHub: [@Ishtiaque-h](https://github.com/Ishtiaque-h)
- LinkedIn: [@ishtiaque-h](https://linkedin.com/in/ishtiaque-h)

---

## ğŸ™ Acknowledgments

- Dataset: [Kaggle â€” Students Performance in Exams](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams)
- Inspiration: Production ML systems at companies like Netflix, Uber, Airbnb
- Tools: FastAPI, scikit-learn, XGBoost, CatBoost, MLflow, GitHub Actions, Google Cloud

---

**Related Projects**: [Boston House Price Prediction](https://github.com/Ishtiaque-h/boston-house-pricing.git)

---

**â­ If you find this project helpful, please consider giving it a star!**